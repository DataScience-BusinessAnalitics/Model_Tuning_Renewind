{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"Train.csv\")  # Replace with actual file path\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Target\"])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imbalanced-learn's Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Preprocess Data\n",
    "def preprocess_data(train, test):\n",
    "    numeric_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Test.csv\")\n",
    "train = pd.read_csv(\"Train.csv\")\n",
    "# all columns are numerical\n",
    "\n",
    "# Robust Scaler is trained with train data\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Select columns (excluding target)\n",
    "num_cols = [col for col in train.columns if col != \"Target\"]\n",
    "\n",
    "train_scaled = train.copy()\n",
    "train_scaled[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "\n",
    "train.fillna(train.median(), inplace=True)\n",
    "test.fillna(train.median(), inplace=True)\n",
    "\n",
    "# Drop highly correlated features from train & test datasets\n",
    "train.drop(columns=['V15', 'V14'], inplace=True)\n",
    "test.drop(columns=['V15', 'V14'], inplace=True) \n",
    "\n",
    "# Separate features and target\n",
    "X_train = train.drop(columns=[\"Target\"])\n",
    "y_train = train[\"Target\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"Target\"])\n",
    "y_test = test[\"Target\"]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random undersampler for under sampling the data\n",
    "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
    "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "best_rf_model_un = RandomForestClassifier(\n",
    "    n_estimators=250,\n",
    "    min_samples_leaf=2,\n",
    "    max_samples=0.6,\n",
    "    max_features=0.1,\n",
    "    random_state=1,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "best_rf_model_un.fit(X_train_un, y_train_un)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1\": f1\n",
    "\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf\n",
    "\n",
    "# Evaluate the final tuned model\n",
    "best_rf_performance = model_performance_classification_sklearn(best_rf_model_un, X_test, y_test)\n",
    "print(\"Final Model Performance on Test Data:\\n\", best_rf_performance)\n",
    "\n",
    "# Export model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_rf_model_un, \"final_rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance:\n",
      "    Accuracy    Recall  Precision        F1\n",
      "0    0.9458  0.875887   0.511387  0.645752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_rf_pipeline.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # For imbalanced data handling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"Train.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "# Define features and target\n",
    "X_train = train.drop(columns=[\"Target\"])\n",
    "y_train = train[\"Target\"]\n",
    "X_test = test.drop(columns=[\"Target\"])\n",
    "y_test = test[\"Target\"]\n",
    "\n",
    "# Identify numerical columns (all columns except 'Target')\n",
    "num_cols = X_train.columns.tolist()\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),  # Handle missing values\n",
    "            (\"scaler\", RobustScaler())  # Scale features\n",
    "        ]), num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the full pipeline\n",
    "final_pipeline = ImbPipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),  # Apply preprocessing (imputation & scaling)\n",
    "    (\"feature_selection\", \"passthrough\"),  # No explicit feature selection, handled manually\n",
    "    (\"undersampling\", RandomUnderSampler(random_state=1, sampling_strategy=1)),  # Balance dataset\n",
    "    (\"classifier\", RandomForestClassifier(\n",
    "        n_estimators=250,\n",
    "        min_samples_leaf=2,\n",
    "        max_samples=0.6,\n",
    "        max_features=0.1,\n",
    "        random_state=1,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train model using pipeline\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using test data\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Define a function to evaluate model performance\n",
    "def evaluate_model(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    return pd.DataFrame({\n",
    "        \"Accuracy\": [accuracy_score(y, pred)],\n",
    "        \"Recall\": [recall_score(y, pred)],\n",
    "        \"Precision\": [precision_score(y, pred)],\n",
    "        \"F1\": [f1_score(y, pred)]\n",
    "    })\n",
    "\n",
    "# Evaluate final model\n",
    "final_performance = evaluate_model(final_pipeline, X_test, y_test)\n",
    "print(\"Final Model Performance:\\n\", final_performance)\n",
    "\n",
    "# Save the trained pipeline\n",
    "joblib.dump(final_pipeline, \"final_rf_pipeline.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
