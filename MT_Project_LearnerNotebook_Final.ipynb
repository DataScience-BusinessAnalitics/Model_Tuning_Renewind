{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "### Business Context\n",
        "\n",
        "Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n",
        "\n",
        "Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n",
        "\n",
        "Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n",
        "\n",
        "The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.).\n",
        "\n",
        "\n",
        "\n",
        "## Objective\n",
        "“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.\n",
        "\n",
        "The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost.\n",
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n",
        "- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n",
        "- False positives (FP) are detections where there is no failure. These will result in inspection costs.\n",
        "\n",
        "It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\n",
        "\n",
        "“1” in the target variables should be considered as “failure” and “0” represents “No failure”.\n",
        "\n",
        "## Data Description\n",
        "- The data provided is a transformed version of original data which was collected using sensors.\n",
        "- Train.csv - To be used for training and tuning of models.\n",
        "- Test.csv - To be used only for testing the performance of the final best model.\n",
        "- Both the datasets consist of 40 predictor variables and 1 target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-uuGqH-qTt"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxVW1EeJ5OI3"
      },
      "outputs": [],
      "source": [
        "# Installing the libraries with the specified version.\n",
        "# %pip install pandas==1.5.3 numpy==1.25.2 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 imbalanced-learn==0.10.1 xgboost==2.0.3 threadpoolctl==3.3.0 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9XTaUAA5Q0M"
      },
      "source": [
        "**Note:** After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83D17_Wl4jal"
      },
      "outputs": [],
      "source": [
        "# To help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# To help with data visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To help with data preprocessing\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# To be used for missing value imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# To help with model building\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# To get different metric scores, and split data\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "# To be used for data scaling and one hot encoding\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, RobustScaler\n",
        "\n",
        "# To be used for tuning the model\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# To be used for creating pipelines and personalizing them\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# To define maximum number of columns to be displayed in a dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# To supress scientific notations for a dataframe\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJnKoHy14jam"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"Test.csv\")\n",
        "train = pd.read_csv(\"Train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpMDcaaMKtI"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIiCRwqZ54_C"
      },
      "source": [
        "- Observations\n",
        "- Sanity checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 39 columns and 1 target column\n",
        "- Columns V1 and V2 contain 18 null values\n",
        "- All columns are numeric values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Class imbalance present: only 5.6% of failures (Target = 1), this means failures are rare.\n",
        "- Some features have extreme min max values indicating possible outliers.\n",
        "- Potential Skewness log transformation or robust scaling possibly needed for highly skewed features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking for duplicates\n",
        "train.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking for missing values\n",
        "round(train.isnull().sum() / train.isnull().count() * 100,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Missing values consist of 0.09% of columns `V1` `V2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01hJQ7EfMKtK"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- There are a total of 5000 rows `V1` and `V2` contain 5 and 4 missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Class imbalance for target variable remains (5.6%) this is consistent across both datasets.\n",
        "- Means and std for most columns are close to the training data.\n",
        "- Outliers exist in both sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checking for duplicates\n",
        "test.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "round(test.isnull().sum() / test.isnull().count() * 100,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `V1` 0.1% missing values `V2` 0.12% missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__7ciGcIDPyk"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWlGr1u1hKyk"
      },
      "source": [
        "### Plotting histograms and boxplots for all the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2op1ZUyhKyk"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jUaMoVvhKyl"
      },
      "source": [
        "### Plotting all the features at one go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W91y_f944jan"
      },
      "outputs": [],
      "source": [
        "for feature in train.columns:\n",
        "    histogram_boxplot(train, feature, figsize=(12, 7), kde=False, bins=None) ## Please change the dataframe name as you define while reading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# Function to detect outliers using IQR\n",
        "def detect_outliers_iqr(df, columns):\n",
        "    outlier_dict = {}\n",
        "    for col in columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "        outlier_dict[col] = len(outliers)\n",
        "    return outlier_dict\n",
        "\n",
        "# Function to detect outliers using Z-score\n",
        "def detect_outliers_zscore(df, columns, threshold=3):\n",
        "    outlier_dict = {}\n",
        "    for col in columns:\n",
        "        z_scores = np.abs(zscore(df[col]))\n",
        "        outliers = df[z_scores > threshold]\n",
        "        outlier_dict[col] = len(outliers)\n",
        "    return outlier_dict\n",
        "\n",
        "# Select numerical columns (excluding target)\n",
        "num_cols = [col for col in train.columns if col != \"Target\"]\n",
        "\n",
        "# Detect outliers\n",
        "iqr_outliers = detect_outliers_iqr(train, num_cols)\n",
        "zscore_outliers = detect_outliers_zscore(train, num_cols)\n",
        "\n",
        "# Display the top features with the most outliers\n",
        "iqr_sorted = sorted(iqr_outliers.items(), key=lambda x: x[1], reverse=True)\n",
        "zscore_sorted = sorted(zscore_outliers.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top Features with Outliers (IQR Method):\", iqr_sorted[:10])\n",
        "print(\"Top Features with Outliers (Z-score Method):\", zscore_sorted[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Failures Relationship to outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Feature with most outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select top feature with most outliers\n",
        "top_feature = iqr_sorted[0][0]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=train[\"Target\"], y=train[top_feature])\n",
        "plt.title(f\"Boxplot of {top_feature} by Failure\")\n",
        "plt.xlabel(\"Failure (0 = No, 1 = Yes)\")\n",
        "plt.ylabel(top_feature)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Feature with second most outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_feature = iqr_sorted[1][0]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=train[\"Target\"], y=train[top_feature])\n",
        "plt.title(f\"Boxplot of {top_feature} by Failure\")\n",
        "plt.xlabel(\"Failure (0 = No, 1 = Yes)\")\n",
        "plt.ylabel(top_feature)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Feature with third most outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_feature = iqr_sorted[3][0]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=train[\"Target\"], y=train[top_feature])\n",
        "plt.title(f\"Boxplot of {top_feature} by Failure\")\n",
        "plt.xlabel(\"Failure (0 = No, 1 = Yes)\")\n",
        "plt.ylabel(top_feature)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- It appears that failures do not have a relationship with outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = RobustScaler()\n",
        "train_scaled = train.copy()\n",
        "train_scaled[num_cols] = scaler.fit_transform(train[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the outliers are not strongly correlated with failures, removing them is unnecessary. Instead, they should be normalized to prevent them from negatively impacting the model.\n",
        "\n",
        "Why RobustScaler\n",
        "\n",
        "- Preserves all data (no removals, no loss of useful information).\n",
        "- Reduces outlier influence (scales using median & IQR instead of mean & standard deviation).\n",
        "- Better for models like Logistic Regression, Decision Trees, and Gradient Boosting, which are sensitive to feature scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the same RobustScaler transformation to the test dataset, but do not fit it again. Instead, use the scaler already fitted on the training data to ensure consistency. \n",
        "\n",
        "If you apply different transformations to the test data, the model will receive differently scaled inputs, leading to incorrect predictions.\n",
        "\n",
        "- Since the test dataset follows a similar distribution to the training set, repeating the analysis is unnecessary. The same preprocessing steps must be applied to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the same scaler (DO NOT FIT AGAIN)\n",
        "test[num_cols] = scaler.transform(test[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "### Missing value imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show only missing values\n",
        "test.isnull().sum()[test.isnull().sum() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show only missing values\n",
        "train.isnull().sum()[train.isnull().sum() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JbJc1bX4jao"
      },
      "outputs": [],
      "source": [
        "train.fillna(train.median(), inplace=True)\n",
        "test.fillna(train.median(), inplace=True)  # Use training set median to avoid data leakage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dealing With class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recommended for boosting and tree-based models\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train[\"Target\"]), y=train[\"Target\"])\n",
        "weights_dict = {0: class_weights[0], 1: class_weights[1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Class Weights:\", weights_dict) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Use class weight to give failures more importance in the model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numerical columns (excluding Target) Re define the num_cols (not necessary)\n",
        "num_cols = [col for col in train.columns if col != \"Target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Correlation Matrix\n",
        "correlation_matrix = train[num_cols].corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the correlation matrix\n",
        "plt.figure(figsize=(20, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws={\"size\": 7})\n",
        "plt.title(\"Correlation Matrix\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify Highly Correlated Features (Threshold: 0.85)\n",
        "correlation_threshold = 0.85\n",
        "highly_correlated_features = set()\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated_features.add(colname)\n",
        "\n",
        "# Print Highly Correlated Features\n",
        "print(\"Highly Correlated Features to Drop:\", highly_correlated_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance using Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(train[num_cols], train[\"Target\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance scores\n",
        "importances = pd.Series(rf.feature_importances_, index=num_cols).sort_values(ascending=False)\n",
        "\n",
        "# Display feature importance as a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "importances.head(15).plot(kind=\"bar\")\n",
        "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Return top 15 most important features\n",
        "importances.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Since `V15` and `V14` are highly correlated with other features, they are dropped to prevent redundancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop highly correlated features from train & test datasets\n",
        "train.drop(columns=['V15', 'V14'], inplace=True)\n",
        "test.drop(columns=['V15', 'V14'], inplace=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Data for Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X_train = train.drop(columns=[\"Target\"])\n",
        "y_train = train[\"Target\"]\n",
        "\n",
        "X_test = test.drop(columns=[\"Target\"])\n",
        "y_test = test[\"Target\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZqmoqz7bp0-"
      },
      "source": [
        "### Model evaluation criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model.\n",
        "- False negatives (FN) are real failures in a generator where there is no detection by model.\n",
        "- False positives (FP) are failure detections in a generator where there is no failure.\n",
        "\n",
        "**Which metric to optimize?**\n",
        "\n",
        "* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.\n",
        "* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.\n",
        "* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQTqGKU4jap"
      },
      "source": [
        "**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIekBxwp4jaq"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Recall\": recall,\n",
        "            \"Precision\": precision,\n",
        "            \"F1\": f1\n",
        "\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hOzddAXbp0_"
      },
      "source": [
        "### Defining scorer to be used for cross-validation and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLUm-eSi_cJ"
      },
      "source": [
        "- We want to reduce false negatives and will try to maximize \"Recall\".\n",
        "- To maximize Recall, we can use Recall as a **scorer** in cross-validation and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayLcyOLsbp0_"
      },
      "outputs": [],
      "source": [
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqCDCbcw4jas"
      },
      "source": [
        "### Model Building with original data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBtuhurlhKyp"
      },
      "source": [
        "Sample Decision Tree model building with original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-tpzI7g4jas"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n",
        "models.append((\"rf\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"ada\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"xgb\", XGBClassifier(random_state=1)))\n",
        "models.append((\"gb\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"bag\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"logistic\", LogisticRegression(random_state=1)))\n",
        "\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=5, shuffle=True, random_state=1\n",
        "    )  # Setting number of splits equal to 5\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores = recall_score(y_test, model.predict(X_test))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Xgboost is the best performing model followed by Decision Tree, all other models show significant drops in validation performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBKJaFU24jas"
      },
      "source": [
        "### Model Building with Oversampled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKxnygkE4jat"
      },
      "outputs": [],
      "source": [
        "# Synthetic Minority Over Sampling Technique\n",
        "sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYDlbnUO4jat"
      },
      "outputs": [],
      "source": [
        "# Train Decision Tree on oversampled data\n",
        "dtree_oversampled = DecisionTreeClassifier(random_state=1)\n",
        "dtree_oversampled.fit(X_train_over, y_train_over)\n",
        "\n",
        "# Evaluate Performance on Test Set\n",
        "performance_df = model_performance_classification_sklearn(dtree_oversampled, X_test, y_test)\n",
        "print(\"Decision Tree:\" '\\n',performance_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=1, class_weight=\"balanced\")\n",
        "rf.fit(X_train_over, y_train_over)\n",
        "\n",
        "rf_performance = model_performance_classification_sklearn(rf, X_test, y_test)\n",
        "print('Random Forest\\n',rf_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest using weights_dict\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=1, class_weight=weights_dict)\n",
        "rf.fit(X_train_over, y_train_over)\n",
        "\n",
        "rf_performance = model_performance_classification_sklearn(rf, X_test, y_test)\n",
        "print('Random Forest using weights_dict\\n',rf_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=1, scale_pos_weight=len(y_train_over) / sum(y_train_over))\n",
        "xgb.fit(X_train_over, y_train_over)\n",
        "\n",
        "xgb_performance = model_performance_classification_sklearn(xgb, X_test, y_test)\n",
        "print('XGBoost\\n',xgb_performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model with the best recall is XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aimb6bn4jat"
      },
      "source": [
        "### Model Building with Undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhxfTkvu4jat"
      },
      "outputs": [],
      "source": [
        "# Random undersampler for under sampling the data\n",
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Decision Tree on undersampled data\n",
        "dtree_undersampled = DecisionTreeClassifier(random_state=1)\n",
        "dtree_undersampled.fit(X_train_un, y_train_un)\n",
        "\n",
        "# Evaluate Performance on Test Set\n",
        "performance_df_under = model_performance_classification_sklearn(dtree_undersampled, X_test, y_test)\n",
        "print('\\n' \"Decision Tree:\" '\\n')\n",
        "print(performance_df_under)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest on undersampled data\n",
        "rf_under = RandomForestClassifier(n_estimators=100, random_state=1, class_weight=\"balanced\")\n",
        "rf_under.fit(X_train_un, y_train_un)\n",
        "\n",
        "rf_performance_under = model_performance_classification_sklearn(rf_under, X_test, y_test)\n",
        "print('Random Forest\\n',rf_performance_under)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest on undersampled data\n",
        "rf_under = RandomForestClassifier(n_estimators=100, random_state=1, class_weight=weights_dict)\n",
        "rf_under.fit(X_train_un, y_train_un)\n",
        "\n",
        "rf_performance_under = model_performance_classification_sklearn(rf_under, X_test, y_test)\n",
        "print('Random Forest with weights_dict\\n',rf_performance_under)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost on undersampled data\n",
        "xgb_under = XGBClassifier(n_estimators=100, random_state=1, scale_pos_weight=len(y_train_un) / sum(y_train_un))\n",
        "xgb_under.fit(X_train_un, y_train_un)\n",
        "\n",
        "xgb_perf_under = model_performance_classification_sklearn(xgb_under, X_test, y_test)\n",
        "print('XGBoost\\n',xgb_perf_under)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model with the best recall Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Overall Undersampled Random Forest (94.7%) has the maximum Recall\n",
        "\n",
        "- Over Sampled Random Forest has a balance of high recall (90.4%) and higher precision meaning less false alarms.\n",
        "\n",
        "Because false alarms have no cost and we are focusing on maximizing recall the model moving forward will be undersampled random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZGY1eL84jau"
      },
      "source": [
        "## HyperparameterTuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxM3jQuK_Pqc"
      },
      "source": [
        "### Sample Parameter Grids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czq7BZ5b4jau"
      },
      "source": [
        "**Hyperparameter tuning can take a long time to run, so to avoid that time complexity - you can use the following grids, wherever required.**\n",
        "\n",
        "- For Gradient Boosting:\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(100,150,25),\n",
        "    \"learning_rate\": [0.2, 0.05, 1],\n",
        "    \"subsample\":[0.5,0.7],\n",
        "    \"max_features\":[0.5,0.7]\n",
        "}\n",
        "\n",
        "- For Adaboost:\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 150, 200],\n",
        "    \"learning_rate\": [0.2, 0.05],\n",
        "    \"base_estimator\": [DecisionTreeClassifier(max_depth=1, random_state=1), DecisionTreeClassifier(max_depth=2, random_state=1), DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ]\n",
        "}\n",
        "\n",
        "- For Bagging Classifier:\n",
        "\n",
        "param_grid = {\n",
        "    'max_samples': [0.8,0.9,1],\n",
        "    'max_features': [0.7,0.8,0.9],\n",
        "    'n_estimators' : [30,50,70],\n",
        "}\n",
        "\n",
        "- For Random Forest:\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200,250,300],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n",
        "}\n",
        "\n",
        "- For Decision Trees:\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': np.arange(2,6),\n",
        "    'min_samples_leaf': [1, 4, 7],\n",
        "    'max_leaf_nodes' : [10, 15],\n",
        "    'min_impurity_decrease': [0.0001,0.001]\n",
        "}\n",
        "\n",
        "- For Logistic Regression:\n",
        "\n",
        "param_grid = {'C': np.arange(0.1,1.1,0.1)}\n",
        "\n",
        "- For XGBoost:\n",
        "\n",
        "param_grid={\n",
        "    'n_estimators': [150, 200, 250],\n",
        "    'scale_pos_weight': [5,10],\n",
        "    'learning_rate': [0.1,0.2],\n",
        "    'gamma': [0,3,5],\n",
        "    'subsample': [0.8,0.9]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMReRXdH_YUd"
      },
      "source": [
        "### Sample tuning method for Decision tree with original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9kks1hG_Xhy"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,6),\n",
        "              'min_samples_leaf': [1, 4, 7],\n",
        "              'max_leaf_nodes' : [10,15],\n",
        "              'min_impurity_decrease': [0.0001,0.001] }\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} \\nwith CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chN8hbfThKyr"
      },
      "source": [
        "### Sample tuning method for Decision tree with oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVZcJ0hv4jau"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,6),\n",
        "              'min_samples_leaf': [1, 4, 7],\n",
        "              'max_leaf_nodes' : [10,15],\n",
        "              'min_impurity_decrease': [0.0001,0.001] }\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over,y_train_over)\n",
        "\n",
        "print(\"Best parameters are {} \\nwith CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtPIiIS7hKyr"
      },
      "source": [
        "### Sample tuning method for Decision tree with undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pbdykhHhKyr"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,20),\n",
        "              'min_samples_leaf': [1, 2, 5, 7],\n",
        "              'max_leaf_nodes' : [5, 10,15],\n",
        "              'min_impurity_decrease': [0.0001,0.001]}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} \\nwith CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Tuning: Undersampled Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Model = RandomForestClassifier(random_state=1)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200,250,300],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [0.3, 0.6, 0.1,'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1),\n",
        "    \"class_weight\": [None, 'balanced', weights_dict]\n",
        "}\n",
        "\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, \n",
        "                                   param_distributions=param_grid, \n",
        "                                   n_iter=10, \n",
        "                                   n_jobs=-1, \n",
        "                                   scoring=scorer, \n",
        "                                   cv=5, \n",
        "                                   random_state=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "randomized_cv.fit(X_train_un, y_train_un)\n",
        "print(\"Best parameters are {} \\nwith CV score={}:\".format(randomized_cv.best_params_, randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_rf_model = RandomForestClassifier(\n",
        "    n_estimators=250,\n",
        "    min_samples_leaf=2,\n",
        "    max_samples=0.6,\n",
        "    max_features=0.1,\n",
        "    random_state=1,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "\n",
        "best_rf_model.fit(X_train_un, y_train_un)\n",
        "\n",
        "# Evaluate the final tuned model\n",
        "best_rf_performance = model_performance_classification_sklearn(best_rf_model, X_test, y_test)\n",
        "print(\"Final Model Performance on Test Data:\\n\", best_rf_performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Tuning: Oversampled Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "randomized_cv.fit(X_train_over, y_train_over)\n",
        "print(\"Best parameters are {} \\nwith CV score={}:\".format(randomized_cv.best_params_, randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_rf_model = RandomForestClassifier(\n",
        "    n_estimators=250,\n",
        "    min_samples_leaf=2,\n",
        "    max_samples=0.6,\n",
        "    max_features=0.1,\n",
        "    random_state=1,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "\n",
        "best_rf_model.fit(X_train_over, y_train_over)\n",
        "\n",
        "best_rf_performance = model_performance_classification_sklearn(best_rf_model, X_test, y_test)\n",
        "print(\"Final Model Performance on Test Data:\\n\", best_rf_performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JNnpxa4jau"
      },
      "source": [
        "## Model performance comparison and choosing the final model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the cost of replacing a generator is the highest, followed by repair costs, and inspections are the least expensive, recall is the most important metric to minimize False Negatives (FN). The Undersampled Random Forest model is the best choice because it has the highest recall (90.8%), meaning it detects the most failures and helps prevent costly replacements, even if it results in more False Positives (FP) and additional inspections. While the Oversampled Random Forest has better precision and F1-score, the main goal is to catch as many failures as possible, since missing them leads to higher costs. Moving forward, the Undersampled RF model should be deployed, with possible threshold tuning to balance recall and false positives. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pDMFAz4jav"
      },
      "source": [
        "### Test set final performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final test set performance for the Undersampled Random Forest model, which was selected based on cost considerations, is:\n",
        "\n",
        "Metric\tFinal Score\n",
        "- Accuracy\t0.707 (70.7%)\n",
        "- Recall (Most Important) \t0.908 (90.8%)\n",
        "- Precision\t0.151 (15.1%)\n",
        "- F1-Score\t0.259 (25.9%)\n",
        "\n",
        "This means the model correctly identifies 90.8% of actual failures, minimizing False Negatives (FN) and preventing costly generator replacements. While precision (15.1%) is low, leading to more False Positives (FP) and extra inspections, the tradeoff is acceptable since inspections are far less expensive than missed failures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM6VZTRn4jav"
      },
      "source": [
        "## Pipelines to build the final model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Data\n",
        "def load_data():\n",
        "    train = pd.read_csv(\"Train.csv\")\n",
        "    test = pd.read_csv(\"Test.csv\")\n",
        "    return train, test\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(train, test):\n",
        "    numeric_features = train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_features = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Model\n",
        "def train_model(train, preprocessor):\n",
        "\n",
        "    print(\"Columns inside function:\", list(train.columns))  # Debugging step\n",
        "\n",
        "    y = train[\"Target\"]\n",
        "    X = train.drop(columns=[\"Target\"])  # Replace 'target' with actual target column\n",
        "    \n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=250, \n",
        "                                              min_samples_leaf=2,\n",
        "                                              max_samples=0.6,\n",
        "                                              max_features=0.1,\n",
        "                                              random_state=1,\n",
        "                                              class_weight=\"balanced\"))\n",
        "    ])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = preprocess_data(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before function call - columns in train: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'Target']\n"
          ]
        }
      ],
      "source": [
        "print(\"Before function call - columns in train:\", list(train.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(train, preprocessor):\n",
        "\n",
        "    print(\"Columns inside function:\", list(train.columns))  # Debugging step\n",
        "\n",
        "    y = train[\"Target\"]\n",
        "    X = train.drop(columns=[\"Target\"])  # Replace 'target' with actual target column\n",
        "    \n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=250, \n",
        "                                              min_samples_leaf=2,\n",
        "                                              max_samples=0.6,\n",
        "                                              max_features=0.1,\n",
        "                                              random_state=1,\n",
        "                                              class_weight=\"balanced\"))\n",
        "    ])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = train_model(train, preprocessor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Run Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    train, test = load_data()\n",
        "    preprocessor = preprocess_data(train, test)\n",
        "    print(train.columns)\n",
        "    model = train_model(train, preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5hPmHyR4jaw"
      },
      "source": [
        "# Business Insights and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB3eO21n_sgt"
      },
      "source": [
        "***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eqCDCbcw4jas",
        "oBKJaFU24jas",
        "1aimb6bn4jat",
        "yZGY1eL84jau",
        "rxM3jQuK_Pqc",
        "GMReRXdH_YUd",
        "chN8hbfThKyr",
        "HtPIiIS7hKyr",
        "D9JNnpxa4jau",
        "d_pDMFAz4jav",
        "TM6VZTRn4jav",
        "c5hPmHyR4jaw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
